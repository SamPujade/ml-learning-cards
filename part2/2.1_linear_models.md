# 2.1. Linear Models

Linear models are the foundation of classical machine learning. They are simple, interpretable, and serve as a building block for many more complex models.

---

## 1. Linear Regression

Linear Regression is a model used for predicting a **continuous** quantitative output.

### 1.1. The Model

It assumes a linear relationship between the input features (X) and the output variable (y).

`y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b`

In vector form: `y = W·X + b`

*   `W`: A vector of weights (or coefficients).
*   `X`: The vector of input features.
*   `b`: The bias term (or intercept).

The goal of training is to find the optimal values for `W` and `b` that best fit the training data.

### 1.2. Loss Function

The most common loss function for Linear Regression is the **Mean Squared Error (MSE)**. It measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.

`MSE = (1/n) * Σ(yᵢ - ŷᵢ)²`

*   `yᵢ`: The actual value.
*   `ŷᵢ`: The predicted value.

### 1.3. Training

The model is trained by minimizing the MSE. This can be done using:
*   **Gradient Descent:** An iterative optimization algorithm that adjusts the weights in the direction that reduces the loss.
*   **Normal Equation:** A closed-form analytical solution that can directly compute the optimal weights. It's computationally expensive for a large number of features.

*   **Interview Insight:** Be ready to discuss the trade-offs between Gradient Descent and the Normal Equation. Gradient Descent scales better to large datasets and many features, while the Normal Equation doesn't require choosing a learning rate and is faster for smaller datasets.

---

## 2. Logistic Regression

Despite its name, Logistic Regression is used for **binary classification**, not regression. It models the probability that an input belongs to a particular class.

### 2.1. The Model

Logistic Regression uses the same linear equation as Linear Regression, but the output is passed through a **Sigmoid (or Logistic) function**.

`z = W·X + b`
`p = sigmoid(z) = 1 / (1 + e⁻ᶻ)`

The sigmoid function squashes the output of the linear equation to a value between 0 and 1, which can be interpreted as a probability.

### 2.2. Loss Function

Because the output is a probability, MSE is not a suitable loss function (it would lead to a non-convex optimization problem). Instead, **Log Loss (or Binary Cross-Entropy)** is used.

`Loss = - (1/n) * Σ [ yᵢ log(pᵢ) + (1 - yᵢ) log(1 - pᵢ) ]`

*   `yᵢ`: The actual class label (0 or 1).
*   `pᵢ`: The predicted probability.

This loss function penalizes the model heavily when it makes a confident but incorrect prediction.

### 2.3. Decision Boundary

A threshold (typically 0.5) is used to convert the predicted probability into a class label.
*   If `p >= 0.5`, predict class 1.
*   If `p < 0.5`, predict class 0.

The decision boundary of a logistic regression model is linear.

*   **Interview Insight:** A key difference from Linear Regression is the interpretation of the output and the choice of the loss function. Emphasize that Logistic Regression models the probability of an outcome and uses a sigmoid function to ensure the output is between 0 and 1. It's a linear classifier, meaning it separates classes with a linear boundary.
