# 2.2. Support Vector Machines (SVM)

Support Vector Machines (SVMs) are a powerful and versatile class of supervised machine learning models used for classification, regression, and outlier detection. They are particularly effective for classification tasks.

---

## 1. The Core Idea: Maximum Margin Classification

The fundamental idea behind SVMs is to find the optimal decision boundary that separates different classes in the feature space.

*   **Hyperplane:** In a 2D space, this is a line. In higher dimensions, it's a hyperplane.
*   **Margin:** The margin is the distance between the hyperplane and the nearest data point from either class.
*   **Support Vectors:** These are the data points that lie closest to the decision boundary. They are the critical elements of the training set, as they are the ones that "support" the hyperplane.

An SVM finds the hyperplane that **maximizes the margin** between the classes. The intuition is that a larger margin leads to a lower generalization error, making the model more robust to new data.

---

## 2. Soft Margin SVM: Handling Non-Separable Data

In real-world scenarios, data is rarely perfectly linearly separable. A "hard margin" SVM would fail in such cases.

The **Soft Margin SVM** addresses this by allowing for some misclassifications. It introduces a hyperparameter, `C`, which controls the trade-off:

*   **Low `C`:** Prioritizes a large margin. The model is more tolerant of misclassifications, leading to a "softer" or wider margin. This can help prevent overfitting.
*   **High `C`:** Prioritizes correctly classifying all training examples. This leads to a "harder" or narrower margin and can lead to overfitting if the data is noisy.

`C` is a crucial hyperparameter that is typically tuned using cross-validation.

---

## 3. The Kernel Trick: Going Non-Linear

SVMs can also model complex, non-linear decision boundaries using the **kernel trick**.

The core idea is to project the data into a higher-dimensional space where it becomes linearly separable. The "trick" is that the SVM algorithm can do this without ever explicitly computing the coordinates of the data in this new, higher-dimensional space. It uses a kernel function to compute the relationships between data points as if they were in the higher-dimensional space.

### Common Kernels:

*   **Linear Kernel:** `K(a, b) = a · b`. This is the default, resulting in a standard linear SVM.
*   **Polynomial Kernel:** `K(a, b) = (γ * a · b + r)ᵈ`. Can model polynomial decision boundaries.
*   **Radial Basis Function (RBF) Kernel:** `K(a, b) = exp(-γ * ||a - b||²)`. A very popular and powerful kernel. It can create complex, non-linear boundaries and is often the default choice. `γ` (gamma) is another important hyperparameter to tune.

---

## 4. Pros and Cons

### Pros:
*   **Effective in high-dimensional spaces:** Works well even when the number of dimensions is greater than the number of samples.
*   **Memory efficient:** Uses only a subset of training points (the support vectors) in the decision function.
*   **Versatile:** Different Kernel functions can be specified for the decision function.

### Cons:
*   **Training time:** Can be slow on very large datasets.
*   **Hyperparameter tuning:** Performance depends heavily on the choice of the kernel and hyperparameters (`C`, `gamma`).
*   **Less interpretable:** The models, especially with non-linear kernels, can be difficult to interpret.

*   **Interview Insight:** The key concepts to highlight are **margin maximization** and the **kernel trick**. Be able to explain how the `C` hyperparameter controls the bias-variance trade-off and how the kernel trick allows a linear classifier to solve non-linear problems.
