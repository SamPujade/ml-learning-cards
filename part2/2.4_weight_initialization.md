# 2.4. Weight Initialization Techniques

## 1. Introduction: The Importance of a Good Start

Training a deep neural network involves finding the optimal values for the weights, starting from some initial guess. The choice of these initial values, a process known as weight initialization, is a critical step that can have a profound impact on the training process. A good initialization can speed up convergence and improve the final performance of the model, while a poor initialization can lead to slow training or prevent the network from learning altogether.

The core challenge is to set the initial weights to values that are not too large and not too small. This is to prevent the activations and gradients from exploding or vanishing as they propagate through the network.

---

## 2. The Problem with Simple Approaches

### 2.1. Initializing to Zero

One might be tempted to initialize all weights to zero. This is a mistake. If all weights are the same, then all neurons in a layer will compute the same output and have the same gradient during backpropagation. This symmetry means that the neurons will all update in the same way, and the network will be unable to learn different features. This is why we need to **break the symmetry** by initializing the weights to random values.

### 2.2. Initializing with Large Random Values

If the weights are initialized to large random values, the inputs to the activation functions (like sigmoid or tanh) can be very large. This pushes the activations into the saturated regions of the function, where the gradients are close to zero. This leads to the **vanishing gradient problem** right from the start, making learning extremely slow.

### 2.3. Initializing with Small Random Values

If the weights are too small, the activations can quickly shrink to zero as they propagate through the network. This also leads to vanishing gradients and prevents the network from learning.

---

## 3. Modern Initialization Techniques

Modern initialization techniques are designed to carefully control the variance of the activations and gradients at each layer. The goal is to keep the variance of the outputs of a layer roughly equal to the variance of its inputs. The choice of initializer is often tied to the choice of activation function.

### 3.1. Xavier/Glorot Initialization

*   **When to use:** This is the recommended initializer when using **sigmoid or tanh** activation functions.
*   **The Idea:** It sets the weights by drawing from a distribution with a mean of 0 and a variance of `1 / n_in`, where `n_in` is the number of input units to the layer. This helps to keep the variance of the activations consistent across layers.
*   **Practical Implementation:**
    *   **Uniform Distribution:** Draw from `U[-limit, limit]`, where `limit = sqrt(6 / (n_in + n_out))`.
    *   **Normal Distribution:** Draw from `N(0, σ²)`, where `σ² = 2 / (n_in + n_out)`.
    *   `n_out` is the number of output units from the layer.

### 3.2. He Initialization

*   **When to use:** This is the recommended initializer when using **ReLU** and its variants (Leaky ReLU, ELU).
*   **The Idea:** The ReLU activation function sets all negative inputs to zero, which means it effectively "kills" half of the activations. This changes the variance of the outputs. He initialization accounts for this by making the initial weights slightly larger. It sets the weights by drawing from a distribution with a mean of 0 and a variance of `2 / n_in`.
*   **Practical Implementation:**
    *   **Uniform Distribution:** Draw from `U[-limit, limit]`, where `limit = sqrt(6 / n_in)`.
    *   **Normal Distribution:** Draw from `N(0, σ²)`, where `σ² = 2 / n_in`.

---

## 4. Practical Guidelines and Interview Insights

*   **Standard Practice:** Modern deep learning frameworks often have sensible default initializers. For example, in PyTorch and TensorFlow/Keras, the linear and convolutional layers are often initialized using a variant of He or Glorot initialization by default.
*   **Biases:** Biases are typically initialized to zero.
*   **Interview Insight:** A key understanding to convey is that weight initialization is not just a random guess; it's a deliberate strategy to control the signal propagation during both the forward and backward passes. Discussing the connection between the choice of activation function and the choice of initializer (e.g., "He for ReLU, Glorot for tanh") demonstrates a practical and theoretical understanding of the challenges of training deep networks. Explaining *why* this is the case (i.e., maintaining variance) shows a deeper level of insight.