# 2.2. Activation Functions

## 1. Introduction: Introducing Non-Linearity

Activation functions are a critical component of any neural network. They are applied to the output of each neuron and are responsible for introducing non-linearity into the model. Without non-linear activation functions, a neural network, no matter how many layers it has, would behave just like a single-layer linear model. This non-linearity is what allows neural networks to learn the complex, non-linear relationships that are common in real-world data.

The choice of activation function for the hidden layers is a key architectural decision that can significantly impact the network's performance and training dynamics.

---

## 2. Common Activation Functions

### 2.1. Sigmoid

*   **Formula:** `σ(x) = 1 / (1 + e⁻ˣ)`
*   **Output Range:** (0, 1)
*   **Description:** The sigmoid function squashes its input into a range between 0 and 1. It was historically popular but is now rarely used in the hidden layers of modern neural networks. It is still used in the output layer for binary classification problems, where its output can be interpreted as a probability.
*   **Problems:**
    1.  **Vanishing Gradients:** The derivative of the sigmoid function is close to zero for very high or very low input values. This means that during backpropagation, the gradients can become very small, effectively "vanishing" as they are propagated backward through the network. This can make it very difficult to train deep networks.
    2.  **Not Zero-Centered:** The output is always positive. This can lead to undesirable zig-zagging dynamics in the gradient updates for the weights.

### 2.2. Tanh (Hyperbolic Tangent)

*   **Formula:** `tanh(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)`
*   **Output Range:** (-1, 1)
*   **Description:** Tanh is a scaled and shifted version of the sigmoid function. It squashes its input into a range between -1 and 1.
*   **Pros:** Being zero-centered, it often performs better than the sigmoid function in hidden layers.
*   **Cons:** It still suffers from the vanishing gradient problem.

### 2.3. ReLU (Rectified Linear Unit)

*   **Formula:** `ReLU(x) = max(0, x)`
*   **Output Range:** [0, ∞)
*   **Description:** The ReLU function is arguably the most important and widely used activation function in deep learning. It is very simple: it outputs the input directly if it is positive, and zero otherwise.
*   **Pros:**
    1.  **No Vanishing Gradient (for positive values):** The derivative is 1 for positive inputs, which helps to mitigate the vanishing gradient problem and allows for faster training.
    2.  **Computational Efficiency:** It is very fast to compute.
    3.  **Sparsity:** Because it outputs zero for negative inputs, it can lead to sparse representations in the hidden layers, which can be desirable.
*   **Cons:**
    1.  **Dying ReLU Problem:** If a neuron's input is always negative, its output will always be zero, and the gradient will also always be zero. This means the neuron effectively "dies" and stops learning.

### 2.4. Leaky ReLU and its Variants

*   **Leaky ReLU Formula:** `f(x) = max(αx, x)`, where `α` is a small constant (e.g., 0.01).
*   **Description:** Leaky ReLU is a modification of ReLU that attempts to solve the dying ReLU problem. Instead of outputting zero for negative inputs, it outputs a small, non-zero, negative value.
*   **Parametric ReLU (PReLU):** `α` is a learnable parameter.
*   **Exponential Linear Unit (ELU):** Uses an exponential curve for negative inputs.
*   **Interview Insight:** These variants often perform slightly better than standard ReLU, but ReLU is still the most common starting point due to its simplicity and good performance in most cases.

### 2.5. Softmax

*   **Formula:** `σ(z)ᵢ = eᶻᵢ / Σ eᶻⱼ` for `j=1...K` classes.
*   **Description:** Softmax is used exclusively in the **output layer** of a multi-class classification network. It takes a vector of arbitrary real-valued scores (logits) and transforms it into a probability distribution over the K classes. The outputs of the softmax function are all between 0 and 1 and sum to 1.

---

## 3. Choosing an Activation Function

*   **For Hidden Layers:**
    *   **Default Choice:** Start with ReLU. It is simple, fast, and usually works well.
    *   **If you have dying neurons:** Try Leaky ReLU or another variant like ELU.
    *   **Avoid:** Sigmoid and Tanh are generally not recommended for hidden layers in deep networks due to the vanishing gradient problem.

*   **For the Output Layer:**
    *   **Binary Classification:** Sigmoid.
    *   **Multi-class Classification:** Softmax.
    *   **Regression:** No activation function (or a linear activation function).