# 3.2. Classic CNN Architectures

## 1. Introduction: The Evolution of Vision Models

The history of deep learning for computer vision is marked by a series of landmark CNN architectures. Each of these models introduced key innovations that significantly advanced the state of the art, often by winning the prestigious ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Understanding the evolution of these architectures provides insight into the core principles of designing effective deep neural networks.

This card reviews the most influential classic CNNs, highlighting their key contributions and architectural patterns.

---

## 2. LeNet-5 (1998)

*   **Contribution:** The pioneering CNN architecture, developed by Yann LeCun for handwritten digit recognition (on the MNIST dataset). It established the fundamental pattern of stacking convolutional and pooling layers.
*   **Architecture:**
    *   A simple sequence of two convolutional layers, two average pooling layers, and two fully connected layers.
    *   Used `tanh` as the activation function.
*   **Interview Insight:** While simple by today's standards, LeNet-5 demonstrated the power of hierarchical feature learning and was a foundational inspiration for later work. It proved that a network could learn to extract spatial features directly from raw pixels.

---

## 3. AlexNet (2012)

*   **Contribution:** The model that kick-started the deep learning revolution by winning the 2012 ImageNet competition with a massive leap in performance. It demonstrated that deep CNNs could achieve unprecedented accuracy on a challenging, large-scale dataset.
*   **Key Innovations:**
    1.  **Deeper Architecture:** It was much deeper than LeNet, with 5 convolutional layers and 3 fully connected layers.
    2.  **ReLU Activation:** It was one of the first successful applications of the ReLU activation function, which was crucial for training a deep network without suffering from the vanishing gradient problem.
    3.  **Dropout:** It used dropout in the fully connected layers to combat overfitting.
    4.  **GPU Training:** It was trained on two GPUs, which was necessary to handle its size and the large ImageNet dataset.
*   **Interview Insight:** AlexNet's success was a powerful proof of concept that convinced many in the research community of the potential of deep learning.

---

## 4. VGGNets (2014)

*   **Contribution:** VGGNet, from the Visual Geometry Group at Oxford, demonstrated that network depth is a critical component of performance. The key idea was to use a very simple and uniform architecture, but to make it very deep.
*   **Architecture:**
    *   Exclusively used small 3x3 convolutional filters, stacked on top of each other. Stacking two 3x3 conv layers has an effective receptive field of a 5x5 layer, and stacking three has a 7x7 receptive field, but with fewer parameters and more non-linearities.
    *   The architecture is very regular, consisting of blocks of 2 or 3 convolutional layers followed by a max pooling layer.
    *   The most famous variants are VGG-16 and VGG-19 (the numbers refer to the number of weight layers).
*   **Interview Insight:** VGG showed that you could achieve excellent performance by simply increasing the depth of the network, as long as you used a consistent and well-designed architecture. Its simplicity and homogeneity made it a very popular baseline model.

---

## 5. ResNet (Residual Network) (2015)

*   **Contribution:** ResNet, from Microsoft Research, tackled the problem of training *extremely* deep networks (over 100 layers). As networks get deeper, they can suffer from the **degradation problem**: accuracy gets saturated and then degrades rapidly. This is not caused by overfitting. ResNet's key innovation was the **residual block**, which allowed for the training of networks that were much deeper than ever before.
*   **The Residual Block:**
    *   Instead of learning a direct mapping from `x` to `H(x)`, the network learns a **residual mapping** `F(x) = H(x) - x`. The output is then `H(x) = F(x) + x`.
    *   This is implemented with a "shortcut" or "skip connection" that bypasses one or more layers and adds the input `x` to the output of the block.
    *   **Why it works:** It is easier for the network to learn to push the residual `F(x)` to zero than it is to learn an identity mapping `H(x) = x`. This means that if a layer is not useful, the network can easily "turn it off" by driving its weights to zero, and the identity connection will simply pass the input through. This makes it possible to train much deeper networks without performance degradation.
*   **Interview Insight:** ResNet was a major breakthrough. The concept of residual connections is now a standard component in many state-of-the-art deep learning architectures, not just for computer vision.