# 9.3. Parameter-Efficient Fine-Tuning (PEFT): LoRA and Beyond

## 1. Introduction: The Need for Efficiency

As Large Language Models (LLMs) grow in size, the computational cost of full fine-tuning becomes prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution to this problem. They aim to reduce the number of trainable parameters, making it possible to fine-tune massive models on consumer hardware.

This card provides a deep dive into PEFT methods, with a focus on Low-Rank Adaptation (LoRA), one of the most popular and effective techniques. We will also explore other PEFT methods and discuss their trade-offs.

---

## 2. LoRA (Low-Rank Adaptation): The Core Idea

LoRA is a PEFT method that injects trainable low-rank matrices into the layers of the Transformer. The core idea is that the change in the weights of the pre-trained model during fine-tuning can be approximated by a low-rank matrix.

**How it works:**

1.  **Freeze the pre-trained model:** The weights of the pre-trained LLM are frozen and are not updated during fine-tuning.
2.  **Inject low-rank matrices:** For each layer of the Transformer that we want to adapt, we inject two low-rank matrices, A and B. The product of these two matrices, AB, is a low-rank approximation of the change in the weights of the layer.
3.  **Train the low-rank matrices:** Only the low-rank matrices A and B are trained during fine-tuning. The number of parameters in A and B is much smaller than the number of parameters in the original layer, which makes the fine-tuning process much more efficient.

**Pros:**

*   Reduces the number of trainable parameters by a factor of up to 10,000.
*   Achieves performance comparable to full fine-tuning on many tasks.
*   Allows for easy task-switching, as we only need to swap out the low-rank matrices for each task.

**Cons:**

*   Introduces a new hyperparameter: the rank of the low-rank matrices.

---

## 3. QLoRA: Quantized Low-Rank Adaptation

QLoRA is an extension of LoRA that further reduces the memory footprint of fine-tuning. It uses a novel method for quantizing the pre-trained model to 4-bit precision, and then uses LoRA to fine-tune the quantized model.

**Key Innovations:**

*   **4-bit NormalFloat (NF4):** A new data type that is information-theoretically optimal for normally distributed weights.
*   **Double Quantization:** A technique for further reducing the memory footprint of the quantized model.
*   **Paged Optimizers:** A method for managing memory spikes during fine-tuning.

**Pros:**

*   Makes it possible to fine-tune massive LLMs (e.g., 65B models) on a single consumer GPU.
*   Achieves performance comparable to full fine-tuning.

---

## 4. Other PEFT Methods

Besides LoRA and QLoRA, there are several other PEFT methods that are worth knowing about:

*   **Prompt Tuning:** This method adds a small number of trainable tokens (a "soft prompt") to the input sequence. The model learns to adapt its behavior based on the soft prompt.
*   **Adapter-based Methods:** These methods insert small, trainable modules (adapters) between the layers of the Transformer. The adapters are trained to adapt the pre-trained model to the downstream task.

**Interview Insight:** Be prepared to discuss the trade-offs between different PEFT methods. For example, LoRA is generally more performant than prompt tuning, but prompt tuning is simpler to implement. The choice of PEFT method will depend on the specific requirements of the task at hand.