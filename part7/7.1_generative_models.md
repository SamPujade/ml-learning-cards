# 7.1. Generative Models: GANs, VAEs, and Diffusion Models

## 1. Introduction: Creating New Data

Generative modeling is a branch of unsupervised learning that focuses on learning the underlying distribution of a dataset in order to generate new, synthetic data points that are similar to the original data. While discriminative models learn to separate data into classes (`p(y|x)`), generative models learn the joint probability `p(x, y)` or the data distribution `p(x)` itself. This allows them to create new data, which has applications in art, data augmentation, drug discovery, and more.

This card covers three of the most influential families of deep generative models: GANs, VAEs, and Diffusion Models.

---

## 2. Generative Adversarial Networks (GANs)

GANs, introduced by Ian Goodfellow in 2014, are based on a clever adversarial, game-theoretic setup. They consist of two neural networks that are trained simultaneously in a zero-sum game:

1.  **The Generator (G):** Takes a random noise vector `z` as input and tries to generate a fake data sample (e.g., an image) that looks like it came from the real dataset. Its goal is to fool the discriminator.
2.  **The Discriminator (D):** A standard binary classifier that takes a data sample (either real or fake) as input and tries to determine whether it is real or fake. Its goal is to correctly identify the generator's fakes.

**The Training Process:**
*   The discriminator is trained on a batch of real images (labeled as "real") and a batch of fake images from the generator (labeled as "fake").
*   The generator is trained to produce images that the discriminator misclassifies as "real". The generator's loss is high if the discriminator correctly identifies its images as fake.

This adversarial process forces the generator to produce increasingly realistic images.

*   **Pros:** Can produce very sharp, high-quality samples.
*   **Cons:** Notoriously difficult and unstable to train. The adversarial dynamic can easily collapse (a state where one network overpowers the other). They don't provide an explicit probability density function.

---

## 3. Variational Autoencoders (VAEs)

VAEs are a generative take on the autoencoder architecture. An autoencoder is trained to reconstruct its input, using an encoder to compress the input into a low-dimensional **latent space** and a decoder to reconstruct the input from the latent representation.

A VAE adds a probabilistic spin to this:

*   **The Encoder (Probabilistic):** Instead of mapping an input to a single point in the latent space, the VAE encoder maps it to a **probability distribution** (typically a Gaussian with a mean `μ` and variance `σ²`).
*   **The Decoder:** To generate a new sample, we first sample a random point `z` from this learned latent distribution and then pass it to the decoder.

**The Loss Function:** VAEs are trained to optimize two objectives simultaneously:
1.  **Reconstruction Loss:** This is the standard autoencoder loss (e.g., MSE) that pushes the model to reconstruct its input accurately.
2.  **KL Divergence:** This is a regularization term that forces the learned latent distributions to be close to a standard normal distribution (`N(0, 1)`). This ensures that the latent space is smooth and continuous, which is essential for generating new, coherent samples.

*   **Pros:** Stable to train. Provide an explicit probability density function. The latent space is often meaningful and can be used for tasks like interpolation between data points.
*   **Cons:** Tend to produce blurrier, lower-quality samples compared to GANs.

---

## 4. Diffusion Models

Diffusion models are the current state-of-the-art for high-quality image generation. They have recently surpassed GANs in terms of sample quality and diversity.

**The Core Idea:** Diffusion models work in two phases:
1.  **Forward Process (Diffusion):** Start with a real image and gradually add a small amount of Gaussian noise over a large number of timesteps. This process slowly destroys the information in the image until, at the final timestep, it is indistinguishable from pure noise. This process is fixed and does not involve any learning.
2.  **Reverse Process (Denoising):** This is the learned part. A neural network (typically a U-Net architecture) is trained to reverse the diffusion process. At each timestep `t`, the model takes the noisy image and predicts the noise that was added between step `t` and `t-1`. By repeatedly subtracting this predicted noise, the model can gradually denoise the image, starting from pure noise and ending with a clean, realistic sample.

**How Generation Works:** To generate a new image, you start with a random noise vector and feed it into the trained denoising network, iteratively applying the denoising process until a clean image is formed.

*   **Pros:** Produce extremely high-quality, diverse samples, often better than GANs. Stable to train.
*   **Cons:** The iterative sampling process is very slow and computationally expensive compared to the single forward pass of a GAN or VAE.

**Interview Insight:** Be able to compare these three families. GANs are a game between a generator and a critic. VAEs are about learning a smooth latent space. Diffusion models are about learning to reverse a noise-adding process. The trend has moved from GANs to Diffusion Models for many state-of-the-art image generation tasks due to their superior sample quality and training stability, despite their computational cost.