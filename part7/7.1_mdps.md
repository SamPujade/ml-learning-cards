# 6.1. Markov Decision Processes (MDPs)

## 1. Introduction: The Formal Framework for Reinforcement Learning

Reinforcement Learning (RL) is about learning to make optimal decisions in an environment to maximize a cumulative reward. Before we can apply learning algorithms, we need a formal way to describe the environment and the decision-making problem. The **Markov Decision Process (MDP)** is the mathematical framework used to model nearly all RL problems.

An MDP provides a way to describe the interactions between a learning agent and its environment in terms of states, actions, and rewards. Understanding MDPs is the first and most crucial step to understanding RL.

---

## 2. The Components of an MDP

An MDP is defined by a tuple of five components: `(S, A, P, R, γ)`

*   **S (Set of States):** A finite set of all possible states the agent can be in. A state is a complete description of the environment at a particular point in time. For example, in a chess game, the state is the position of all the pieces on the board.

*   **A (Set of Actions):** A finite set of all possible actions the agent can take. For a given state `s`, `A(s)` is the set of actions available in that state.

*   **P (Transition Probability Function):** `P(s' | s, a)` defines the dynamics of the environment. It is the probability of transitioning to state `s'` after taking action `a` in state `s`. The environment may be stochastic, meaning that taking the same action in the same state may not always lead to the same next state.

*   **R (Reward Function):** `R(s, a, s')` defines the immediate reward the agent receives after transitioning from state `s` to state `s'` as a result of taking action `a`. The agent's goal is to maximize the total reward it accumulates over time, not just the immediate reward.

*   **γ (Discount Factor):** A number between 0 and 1 that determines the importance of future rewards. A reward received `k` timesteps in the future is discounted by a factor of `γ^k`.
    *   If `γ = 0`, the agent is "myopic" and only cares about maximizing the immediate reward.
    *   If `γ` is close to 1, the agent is "farsighted" and takes future rewards into account.
    *   **Interview Insight:** The discount factor is mathematically necessary to prevent the total reward from becoming infinite in environments with no terminal state. It also reflects a degree of uncertainty about the future; a reward in the distant future is often less certain than an immediate one.

---

## 3. The Markov Property

The "Markov" in MDP refers to the **Markov Property**. This is a critical assumption that states that the future is independent of the past, given the present.

`P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1} | S_t, A_t)`

In other words, the current state `S_t` must be **sufficient** to capture all relevant information from the history. The agent doesn't need to know the entire history of states and actions it has taken; all the information needed to make an optimal decision is contained within the current state.

**Interview Insight:** The Markov property is a powerful simplifying assumption. While it may not hold perfectly in all real-world scenarios (these are called partially observable MDPs, or POMDPs), it is a very effective model for a wide range of problems. In many cases, we can design the state representation to include enough information from the past (e.g., the last few video frames) to make the Markov assumption a reasonable approximation.

---

## 4. The Goal: Finding the Optimal Policy

The solution to an MDP is a **policy**, denoted by `π`. A policy is a mapping from states to actions. It tells the agent what action to take in any given state.

`π(a | s) = P(A_t = a | S_t = s)`

The goal of a reinforcement learning agent is to find the **optimal policy**, `π*`, which is the policy that maximizes the expected cumulative discounted reward (the "return"). The core of all RL algorithms is about finding this optimal policy. The next cards will discuss the different families of algorithms for finding `π*`.