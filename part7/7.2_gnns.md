# 7.2. Graph Neural Networks (GNNs)

## 1. Introduction: Machine Learning on Graphs

Many real-world datasets are best represented as graphs. Social networks, molecular structures, citation networks, and knowledge bases are all examples of data where the relationships and connections between entities are just as important as the entities themselves. Traditional deep learning models like CNNs and RNNs are designed for data with a regular, grid-like structure and struggle to handle the arbitrary structure of graphs.

Graph Neural Networks (GNNs) are a class of neural networks specifically designed to perform inference on data described by graphs. They can learn from the graph's structure, the features of its nodes, and the features of its edges.

---

## 2. The Core Idea: Message Passing

The fundamental idea behind most GNN architectures is **message passing** or **neighborhood aggregation**. The goal is to learn a representation (embedding) for each node in the graph. This is done iteratively, where each node's representation is updated by aggregating information from its neighbors.

A single layer of a GNN performs the following two steps for every node in the graph:

1.  **Message Aggregation:** Each node collects feature vectors (or "messages") from its immediate neighbors. This is the "neighborhood aggregation" step. The aggregation function must be permutation-invariant, meaning it doesn't matter in which order the neighbors are processed. Common aggregation functions include:
    *   Sum
    *   Mean
    *   Max

2.  **Update:** Each node updates its own feature vector using the aggregated message from its neighbors and its own previous feature vector. This update step is typically performed by a small neural network (e.g., a single linear layer followed by a non-linearity).

By stacking multiple GNN layers, a node's representation can incorporate information from nodes that are further and further away in the graph. A `k`-layer GNN can aggregate information from a node's `k`-hop neighborhood.

---

## 3. A Simple GNN Layer

Let `h_v^(k)` be the feature vector (embedding) of node `v` at layer `k`. A single GNN layer updates this embedding to `h_v^(k+1)` as follows:

`a_v^(k) = AGGREGATE({h_u^(k) for u in N(v)})`
`h_v^(k+1) = UPDATE(h_v^(k), a_v^(k))`

*   `N(v)` is the set of neighbors of node `v`.
*   `AGGREGATE` is the aggregation function (e.g., sum, mean, max).
*   `UPDATE` is the update function (e.g., a neural network).

A common and simple formulation for the update rule is:
`h_v^(k+1) = σ(W * [h_v^(k), a_v^(k)])`
where `W` is a learnable weight matrix, `[...]` denotes concatenation, and `σ` is a non-linear activation function.

---

## 4. Common GNN Tasks

GNNs can be used for several types of tasks on graphs:

*   **Node Classification:** Predict a property of a single node. The final embedding for each node is passed to a classifier.
    *   *Example:* Classifying a user in a social network as a bot or a human.

*   **Graph Classification:** Predict a property of the entire graph. To do this, all the final node embeddings must be aggregated into a single graph-level representation (e.g., by summing or averaging them). This graph embedding is then passed to a classifier.
    *   *Example:* Classifying a molecule as toxic or non-toxic.

*   **Link Prediction:** Predict whether there is an edge between two nodes. The final embeddings of the two nodes are combined (e.g., via a dot product) and passed to a classifier.
    *   *Example:* Recommending new friends in a social network.

---

## 5. Popular GNN Variants

*   **Graph Convolutional Network (GCN):** A specific and very popular type of GNN that uses a weighted average of the neighbor features as its aggregation function. The weights are determined by the degrees of the nodes, providing a form of normalization.
*   **GraphSAGE:** (Graph SAmple and aggreGatE) Generalizes the GCN by allowing for different aggregation functions (e.g., mean, max, or even an LSTM). It also introduces a way to train on large graphs by sampling a fixed-size neighborhood for each node instead of using all neighbors.
*   **Graph Attention Network (GAT):** Incorporates the attention mechanism into the aggregation step. Instead of treating all neighbors equally (like in a GCN or a mean-aggregator), GAT learns to assign different attention weights to different neighbors, allowing the model to focus on the most relevant ones.

**Interview Insight:** The key concept of GNNs is learning node representations by iteratively aggregating information from their local neighborhoods. This "message passing" scheme is a powerful way to encode both the node features and the graph structure into a dense vector representation. Being able to explain how stacking layers allows a GNN to "see" further into the graph is crucial. Mentioning the different tasks (node, graph, link prediction) and popular variants like GCN and GAT will demonstrate a comprehensive understanding of the topic.