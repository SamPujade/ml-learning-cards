# 1.4. Optimization Algorithms

## 1. Introduction: Finding the Minimum

At the heart of most machine learning models is an optimization problem. We define a loss function that measures how well the model's predictions match the true labels, and our goal is to find the set of model parameters (weights) that minimizes this loss. Optimization algorithms are the methods we use to navigate the "loss landscape" and find this minimum.

The choice of optimization algorithm can have a significant impact on both the speed of training and the final performance of the model. While Gradient Descent is the foundational concept, more advanced algorithms have been developed to handle the challenges of training large, complex models, such as those found in deep learning.

---

## 2. Gradient Descent and its Variants

The core idea behind gradient-based optimization is to iteratively update the model's parameters in the direction opposite to the gradient of the loss function. The gradient is a vector that points in the direction of the steepest ascent of the loss function, so moving in the opposite direction takes us closer to a minimum.

`w_new = w_old - η * ∇Loss(w)`

where `η` is the learning rate, a hyperparameter that controls the step size.

### 2.1. Batch Gradient Descent

*   **How it works:** Batch Gradient Descent calculates the gradient of the loss function with respect to the entire training dataset in each update step.
*   **Pros:** Guarantees convergence to the global minimum for convex loss surfaces and to a local minimum for non-convex surfaces. The gradient calculation is stable.
*   **Cons:** Very slow and computationally expensive for large datasets, as it requires the entire dataset to be in memory. It is not suitable for online learning.

### 2.2. Stochastic Gradient Descent (SGD)

*   **How it works:** SGD updates the parameters after calculating the gradient on just a *single* randomly chosen training example.
*   **Pros:** Much faster than Batch GD. It can be used for online learning. The noisy updates can help the model escape shallow local minima.
*   **Cons:** The updates are very noisy, leading to a less stable convergence path. The frequent updates can be computationally inefficient.

### 2.3. Mini-Batch Gradient Descent

*   **How it works:** Mini-Batch GD is a compromise between Batch GD and SGD. It updates the parameters after calculating the gradient on a small subset (a "mini-batch") of the training data.
*   **Pros:** Offers a balance between the stability of Batch GD and the speed of SGD. It is the most common variant of gradient descent used in deep learning. It allows for efficient computation using vectorized operations.
*   **Cons:** Introduces a new hyperparameter: the batch size.

---

## 3. Advanced Optimization Algorithms

While Mini-Batch GD is effective, several algorithms have been developed to improve its convergence properties. These are known as adaptive optimization algorithms because they adapt the learning rate during training.

### 3.1. SGD with Momentum

*   **Concept:** Momentum helps accelerate SGD in the relevant direction and dampens oscillations. It adds a fraction of the previous update vector to the current one. This simulates the concept of "momentum" in physics, where a ball rolling down a hill gathers speed.
*   **Interview Insight:** Explain how momentum helps the optimizer to power through flat regions and navigate ravines (areas where the loss surface is much steeper in one dimension than another) more effectively.

### 3.2. RMSprop

*   **Concept:** RMSprop (Root Mean Square Propagation) is another adaptive learning rate algorithm. It divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. This has the effect of decreasing the learning rate for weights with large gradients and increasing it for weights with small gradients.
*   **Interview Insight:** Explain that RMSprop is particularly useful in settings with non-stationary objectives (i.e., the loss landscape changes over time), and it is one of the key components of the Adam optimizer.

### 3.3. Adam (Adaptive Moment Estimation)

*   **Concept:** Adam is one of the most popular and effective optimization algorithms. It combines the ideas of both Momentum and RMSprop. It computes adaptive learning rates for each parameter by keeping track of two "moments" of the gradients:
    1.  The first moment (the mean of the gradients), similar to momentum.
    2.  The second moment (the uncentered variance of the gradients).
*   **Pros:** Adam is computationally efficient, has low memory requirements, and is well-suited for problems with large datasets and high-dimensional parameter spaces. It often works well with little hyperparameter tuning.
*   **Interview Insight:** Be prepared to discuss why Adam is often the default choice for deep learning tasks. Also, mention potential pitfalls, such as its performance being sensitive to the choice of its hyperparameters (beta1, beta2, epsilon) in some cases.

---

## 4. The Role of the Learning Rate

The learning rate (`η`) is arguably the most important hyperparameter for these optimization algorithms.
*   **Too small:** The model will learn very slowly and may get stuck in a local minimum.
*   **Too large:** The optimizer may overshoot the minimum and fail to converge, with the loss oscillating or even diverging.

**Learning Rate Schedules:** It is common practice to use a learning rate schedule, where the learning rate is decreased over the course of training. This allows the optimizer to take large steps at the beginning of training and smaller, more refined steps as it gets closer to the minimum.
