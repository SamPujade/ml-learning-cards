# 1.5. Evaluation Metrics for Classification & Regression

## 1. Introduction: How Good is My Model?

Building a machine learning model is only half the battle; the other half is evaluating its performance. Evaluation metrics provide a quantitative way to measure how well a model is doing and to compare different models. The choice of evaluation metric is crucial and depends heavily on the specific business problem you are trying to solve. A model that is "good" by one metric might be "bad" by another, so understanding the nuances of these metrics is essential for delivering a model that provides real value.

This card covers the most important evaluation metrics for the two main types of supervised learning problems: classification and regression.

---

## 2. Metrics for Classification Problems

Classification models predict a categorical label. The choice of metric often depends on the class distribution (balanced vs. imbalanced) and the relative importance of different types of errors.

### 2.1. The Confusion Matrix

The confusion matrix is the foundation for most other classification metrics. It is a table that summarizes the performance of a classification model by showing the counts of true and false predictions for each class.

|                | Predicted Positive | Predicted Negative |
|----------------|--------------------|--------------------|
| Actual Positive| True Positive (TP) | False Negative (FN)|
| Actual Negative| False Positive (FP)| True Negative (TN) |

*   **TP:** Correctly predicted positive.
*   **TN:** Correctly predicted negative.
*   **FP (Type I Error):** Incorrectly predicted positive (e.g., a non-spam email is classified as spam).
*   **FN (Type II Error):** Incorrectly predicted negative (e.g., a spam email is classified as not spam).

### 2.2. Core Classification Metrics

*   **Accuracy:** `(TP + TN) / (TP + TN + FP + FN)`
    *   The most intuitive metric, it measures the proportion of correct predictions.
    *   **Interview Insight:** Accuracy can be very misleading for imbalanced datasets. For example, if 99% of emails are not spam, a model that predicts "not spam" every time will have 99% accuracy but will be useless.

*   **Precision:** `TP / (TP + FP)`
    *   Of all the positive predictions, how many were actually correct?
    *   High precision is important when the cost of a False Positive is high (e.g., in medical diagnosis, you don't want to tell a healthy person they have a disease).

*   **Recall (Sensitivity, True Positive Rate):** `TP / (TP + FN)`
    *   Of all the actual positive cases, how many did the model correctly identify?
    *   High recall is important when the cost of a False Negative is high (e.g., in fraud detection, you don't want to miss a fraudulent transaction).

*   **F1-Score:** `2 * (Precision * Recall) / (Precision + Recall)`
    *   The harmonic mean of precision and recall. It provides a single score that balances both concerns. It is a good choice when you want to find a balance between precision and recall, especially in the context of an imbalanced dataset.

### 2.3. ROC Curve and AUC

*   **ROC (Receiver Operating Characteristic) Curve:** A plot of the True Positive Rate (Recall) against the False Positive Rate (`FP / (FP + TN)`) at various classification thresholds. It shows how well the model can distinguish between the positive and negative classes.
*   **AUC (Area Under the ROC Curve):** The area under the ROC curve.
    *   **Interpretation:** AUC represents the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance. An AUC of 1.0 is a perfect classifier, while an AUC of 0.5 is equivalent to a random classifier.
    *   **Interview Insight:** AUC is a very popular metric because it is classification-threshold-invariant and provides a single number to summarize model performance.

---

## 3. Metrics for Regression Problems

Regression models predict a continuous value. Evaluation metrics for regression measure the average error or difference between the predicted values and the actual values.

*   **Mean Absolute Error (MAE):** `(1/n) * Σ|y_true - y_pred|`
    *   The average of the absolute differences between the predictions and the actual values.
    *   **Pros:** It is easy to interpret, as it is in the same units as the target variable. It is less sensitive to outliers than MSE.

*   **Mean Squared Error (MSE):** `(1/n) * Σ(y_true - y_pred)²`
    *   The average of the squared differences between the predictions and the actual values.
    *   **Pros:** It penalizes larger errors more heavily than smaller ones due to the squaring. This can be desirable if large errors are particularly bad for the business problem.
    *   **Cons:** The units are the square of the target variable's units, which can make it harder to interpret. It is sensitive to outliers.

*   **Root Mean Squared Error (RMSE):** `sqrt(MSE)`
    *   The square root of the MSE.
    *   **Pros:** It is in the same units as the target variable, making it easier to interpret than MSE. It still penalizes large errors more heavily than MAE.
    *   **Interview Insight:** RMSE is one of the most common regression metrics. Be prepared to discuss the MAE vs. RMSE tradeoff: if you want to treat all errors equally, use MAE. If you want to penalize large errors more, use RMSE.

*   **R-squared (Coefficient of Determination):**
    *   Measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).
    *   **Interpretation:** An R-squared of 1 indicates that the model explains all the variability of the response data around its mean. An R-squared of 0 indicates that the model explains none of the variability.
    *   **Interview Insight:** While popular, R-squared can be misleading. It will always increase as you add more features to the model, even if those features are not useful. This is why Adjusted R-squared, which penalizes the score for extra features, is often preferred.
