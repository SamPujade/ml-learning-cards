# 5.2. Self-Attention and Multi-Head Attention

## 1. Introduction: Attention on Itself

The original attention mechanism was used in seq2seq models to help a decoder focus on an encoder's output. The breakthrough of the Transformer model was to get rid of the recurrent layers entirely and use a specific kind of attention called **self-attention**.

Self-attention is a mechanism that allows the words in a single sequence to interact with each other and figure out which other words they should pay attention to. It enables the model to build a rich, context-aware representation of each word by looking at the entire sequence. For example, in the sentence "The animal didn't cross the street because it was too tired," self-attention can learn that the word "it" refers to "animal" and not "street."

---

## 2. Self-Attention: The QKV Mechanism Revisited

Self-attention works by applying the Query-Key-Value (QKV) model to a single input sequence. For each word in the sequence, we create a Query, a Key, and a Value.

1.  **Create Q, K, V Vectors:**
    *   Start with the input embedding for each word in the sequence.
    *   Create three separate weight matrices: `W_Q`, `W_K`, and `W_V`. These matrices are learned during training.
    *   For each word's embedding `x_i`, create its Query, Key, and Value vectors by multiplying it with these weight matrices:
        *   `q_i = W_Q * x_i`
        *   `k_i = W_K * x_i`
        *   `v_i = W_V * x_i`

2.  **Calculate Scores:** To get the new representation for a specific word `i`, we take its **Query** `q_i` and score it against the **Key** of every other word in the sequence, including itself. This is typically done with a dot product.
    `score(i, j) = q_iᵀk_j`

3.  **Scale and Softmax:** The scores are then scaled by dividing by the square root of the dimension of the key vectors (`d_k`). This is done to prevent the dot products from growing too large and pushing the softmax function into regions with very small gradients. A softmax is then applied to get the attention weights.
    `weight(i, j) = softmax(score(i, j) / sqrt(d_k))`

4.  **Compute the Output:** The new representation for word `i`, `z_i`, is the weighted sum of all the **Value** vectors in the sequence.
    `z_i = Σ (weight(i, j) * v_j)`

This process is performed in parallel for every word in the sequence to produce a new sequence of context-aware representations.

---

## 3. Multi-Head Attention: Attending to Different Things

It can be restrictive to expect a single self-attention mechanism to capture all the different types of relationships between words. For example, one attention pattern might capture syntactic relationships, while another might capture semantic ones.

**Multi-Head Attention** allows the model to jointly attend to information from different representation subspaces at different positions. It does this by running the self-attention mechanism multiple times in parallel and then combining the results.

**The Process:**

1.  **Create Multiple Q, K, V Sets:** Instead of just one set of `W_Q`, `W_K`, and `W_V` matrices, we create `h` different sets (where `h` is the number of "heads"). Each set is initialized randomly and will learn to project the input embeddings into a different representation subspace.

2.  **Run Self-Attention in Parallel:** We run the self-attention mechanism `h` times, once for each head, producing `h` separate output vectors (`z_i` for each head). Each head can "attend" to different parts of the input.

3.  **Concatenate and Project:** The `h` output vectors are concatenated together and then passed through a final linear projection layer (with weight matrix `W_O`) to produce the final output of the Multi-Head Attention layer.

**Interview Insight:** The key advantage of Multi-Head Attention is that it gives the model more flexibility to capture different kinds of relationships. Each attention head can specialize in a different type of interaction. For example, one head might learn to track subject-verb relationships, while another tracks pronoun-antecedent relationships. This allows for a much richer and more nuanced understanding of the language than a single attention mechanism could achieve.