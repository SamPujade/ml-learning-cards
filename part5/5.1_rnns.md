# 4.1. Recurrent Neural Networks (RNNs) & The Vanishing/Exploding Gradient Problem

## 1. Introduction: Handling Sequential Data

While CNNs are designed for spatial data, Recurrent Neural Networks (RNNs) are a class of neural networks specialized for processing sequential data. This includes time series data, natural language, audio, and video. The defining feature of an RNN is its ability to maintain an internal state, or "memory," which allows it to capture information about the preceding elements in a sequence.

Unlike a standard feedforward network, which processes each input independently, an RNN performs the same computation for every element of a sequence, and the output for each element is dependent on the previous computations. This "recurrent" nature is what gives RNNs their name and their power.

---

## 2. The Architecture of a Simple RNN

A simple RNN can be thought of as a standard neural network with a feedback loop. For each timestep `t` in a sequence, the RNN cell takes two inputs:

1.  **The input at the current timestep:** `x_t`
2.  **The hidden state from the previous timestep:** `h_{t-1}`

It then computes the output for the current timestep, `y_t`, and the new hidden state, `h_t`, which is then passed to the next timestep.

**The Recurrent Formula:**

`h_t = g(W_hh * h_{t-1} + W_xh * x_t + b_h)`
`y_t = f(W_hy * h_t + b_y)`

*   `h_t` is the new hidden state.
*   `h_{t-1}` is the previous hidden state.
*   `x_t` is the input at timestep `t`.
*   `W_hh`, `W_xh`, and `W_hy` are the weight matrices that are **shared across all timesteps**. This parameter sharing is a key feature of RNNs.
*   `g` and `f` are activation functions (e.g., `tanh` or `ReLU`).

**Unrolling the RNN:** To visualize the computation, an RNN is often "unrolled" in time. This means we create a copy of the RNN cell for each timestep in the sequence, with the hidden state being passed from one copy to the next.

---

## 3. The Vanishing and Exploding Gradient Problem

Training an RNN involves a modified version of backpropagation called **Backpropagation Through Time (BPTT)**. Because the same weight matrix (`W_hh`) is applied at every timestep, the gradients are calculated by summing up the contributions from each timestep.

During BPTT, the gradient signal has to travel backward from the end of the sequence to the beginning. The issue arises from the repeated multiplication by the recurrent weight matrix `W_hh`.

*   **Vanishing Gradients:** If the values in `W_hh` are small (or, more formally, if the leading eigenvalue of `W_hh` is less than 1), the gradient signal can shrink exponentially as it propagates backward through time. This means that the network is unable to learn long-range dependencies; the gradients from distant timesteps become so small that they have no effect on the weight updates. This is the primary reason why simple RNNs struggle with long sequences.

*   **Exploding Gradients:** If the values in `W_hh` are large (leading eigenvalue > 1), the gradient signal can grow exponentially, leading to huge, unstable weight updates. The loss can become `NaN` (Not a Number), and the training process fails.

**Interview Insight:** The vanishing gradient problem is the more common and insidious of the two. It's the fundamental limitation that motivated the development of more advanced recurrent architectures like LSTMs and GRUs.

---

## 4. Mitigating the Problem

*   **Exploding Gradients:** This problem is easier to solve. A common technique is **gradient clipping**. If the norm of the gradient vector exceeds a certain threshold, it is scaled down to a manageable size before the weight update. This prevents the updates from becoming too large.

*   **Vanishing Gradients:** This is a much harder problem to solve for simple RNNs. The most effective solutions involve changing the architecture of the RNN cell itself.
    *   **Activation Functions:** Using ReLU instead of `tanh` can help, but it doesn't solve the core problem.
    *   **Gated Architectures:** The most successful solution is to introduce "gating" mechanisms that explicitly control the flow of information through the network. These gates can learn to "remember" important information from the past and "forget" irrelevant information. This is the core idea behind **LSTMs** and **GRUs**, which are the subject of the next cards.