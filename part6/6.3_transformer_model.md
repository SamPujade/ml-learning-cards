# 5.3. The Transformer Encoder-Decoder Model

## 1. Introduction: "Attention Is All You Need"

The Transformer, introduced in the 2017 paper "Attention Is All You Need," is a landmark network architecture that has revolutionized NLP and is now being applied to many other domains. Its core innovation was to completely dispense with recurrence and convolutions and rely entirely on self-attention mechanisms to process sequential data.

This allows for significantly more parallelization than RNNs (which are inherently sequential) and enables the model to learn long-range dependencies more effectively. The original Transformer was designed for machine translation and consists of an encoder-decoder architecture.

---

## 2. The Overall Architecture

The Transformer follows a classic encoder-decoder structure.

*   **Encoder:** The encoder's job is to take the input sequence and build a rich, contextualized representation of it. It is composed of a stack of `N` identical encoder layers.
*   **Decoder:** The decoder's job is to take the encoder's output and generate the output sequence, one word at a time. It is composed of a stack of `N` identical decoder layers.

### The Missing Piece: Positional Encodings

Since the self-attention mechanism is permutation-invariant (it doesn't have a built-in sense of word order), we need a way to inject information about the position of each word in the sequence. This is done by adding a **positional encoding** vector to each input embedding. These encodings are pre-calculated vectors (using sine and cosine functions of different frequencies) that give the model information about the absolute or relative position of words.

---

## 3. The Encoder Block

Each of the `N` encoder layers has two main sub-layers:

1.  **Multi-Head Self-Attention:** This is the self-attention mechanism described in the previous card. It allows each word in the input sequence to attend to all other words, creating a context-aware representation.
2.  **Position-wise Feed-Forward Network (FFN):** This is a simple, fully connected feed-forward network that is applied to each position (each word's representation) separately and identically. It consists of two linear transformations with a ReLU activation in between. This FFN adds further processing and non-linearity to the model.

**Residuals and Normalization:** Each of these two sub-layers has a **residual connection** around it, followed by a **layer normalization** step.
`Output = LayerNorm(x + Sublayer(x))`
This is crucial for enabling the training of very deep Transformer models.

---

## 4. The Decoder Block

Each of the `N` decoder layers is similar to the encoder layer but has **three** main sub-layers:

1.  **Masked Multi-Head Self-Attention:** The decoder is auto-regressive, meaning it generates the output sequence one word at a time, using the words it has already generated. To prevent the decoder from "cheating" by looking at future words in the output sequence during training, we use a **look-ahead mask**. This mask is applied to the self-attention scores, setting the scores for all future positions to negative infinity. This ensures that the prediction for position `i` can depend only on the known outputs at positions less than `i`.

2.  **Encoder-Decoder Attention:** This is the key layer that connects the encoder and the decoder. It works just like the multi-head attention mechanism, but with one crucial difference:
    *   The **Queries** come from the output of the previous decoder sub-layer.
    *   The **Keys and Values** come from the final output of the **encoder stack**.
    This allows every position in the decoder to attend to all positions in the input sequence, just like the original attention mechanism in seq2seq models.

3.  **Position-wise Feed-Forward Network:** This is identical to the FFN in the encoder.

Like the encoder, each sub-layer in the decoder also has a residual connection and a layer normalization step.

---

## 5. Final Output

After the final decoder layer, there is a final linear layer followed by a softmax function to produce the probability distribution over the vocabulary for the next word in the output sequence.

**Interview Insight:** Be able to articulate the data flow. The encoder processes the entire input sequence at once. The decoder then generates the output sequence token by token. For each token, it performs masked self-attention on the output generated so far, then performs encoder-decoder attention on the encoder's output, and finally passes the result through an FFN to predict the next token. The non-sequential nature of the encoder is what makes the Transformer so parallelizable and efficient compared to RNNs.