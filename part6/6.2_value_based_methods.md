# 6.2. Value-Based Methods (Q-Learning, DQN)

## 1. Introduction: Learning the Value of Actions

Value-based methods are a cornerstone of reinforcement learning. Instead of directly learning a policy (a mapping from states to actions), these methods learn a **value function**. A value function estimates the expected return (cumulative discounted reward) from being in a certain state, or from taking a certain action in a certain state. Once the optimal value function is learned, the optimal policy can be derived by simply choosing the action that leads to the highest value in any given state.

The most important concept in value-based RL is the **action-value function**, denoted `Q(s, a)`. This function represents the expected return after taking action `a` in state `s` and then following the optimal policy thereafter.

---

## 2. Q-Learning: The Classic Value-Based Algorithm

Q-learning is a classic **off-policy**, **model-free** RL algorithm.
*   **Model-free:** It doesn't need to know the transition probabilities `P` or the reward function `R` of the MDP. It learns directly from experience.
*   **Off-policy:** It can learn the optimal policy even while it is executing a different, exploratory policy.

**The Core Idea:** Q-learning aims to learn the optimal Q-function, `Q*(s, a)`. It does this by iteratively updating its estimate of `Q(s, a)` based on the agent's interactions with the environment. The update rule is derived from the Bellman equation:

`Q(s, a) ← Q(s, a) + α * [r + γ * max_{a'} Q(s', a') - Q(s, a)]`

Let's break this down:
*   `Q(s, a)`: The current estimate of the value of taking action `a` in state `s`.
*   `α`: The learning rate.
*   `r`: The immediate reward received after taking action `a`.
*   `γ`: The discount factor.
*   `s'`: The next state.
*   `max_{a'} Q(s', a')`: This is the crucial part. It is the estimate of the optimal future value from the next state `s'`. We look at all possible actions `a'` from state `s'` and take the maximum Q-value.
*   `[r + γ * max_{a'} Q(s', a') - Q(s, a)]`: This is the **Temporal Difference (TD) error**. It's the difference between our new, updated estimate of the value of `(s, a)` and our old estimate. We are updating our current belief based on the new information we received.

In tabular Q-learning (for small, discrete state spaces), we simply maintain a table of Q-values for every state-action pair.

---

## 3. Deep Q-Networks (DQN): Q-Learning with Neural Networks

Tabular Q-learning is not feasible for problems with large or continuous state spaces (like learning to play Atari games from raw pixel data). This is where deep learning comes in. A Deep Q-Network (DQN) is a neural network that is trained to approximate the Q-function, `Q(s, a; θ)`, where `θ` are the weights of the network.

The DQN algorithm, introduced by DeepMind in 2013, was a major breakthrough that combined Q-learning with deep neural networks to achieve human-level performance on a range of Atari games. It introduced several key innovations to stabilize the training process.

### 3.1. Experience Replay

*   **The Problem:** Training a neural network requires the data to be independent and identically distributed (i.i.d.). However, the sequence of states and actions generated by an RL agent is highly correlated.
*   **The Solution:** The agent stores its experiences `(s, a, r, s')` in a large buffer called a **replay memory**. During training, it samples random mini-batches of experiences from this buffer to update the network. This breaks the temporal correlations and makes the training more stable and efficient.

### 3.2. Fixed Q-Targets

*   **The Problem:** In the Q-learning update, we are using the same network to both predict the Q-value for the current state `Q(s, a)` and to estimate the target value `max_{a'} Q(s', a')`. This means the target is constantly changing, which can lead to oscillations and instability.
*   **The Solution:** Use two separate networks.
    1.  **Policy Network:** The main network that we are training.
    2.  **Target Network:** A clone of the policy network. Its weights are frozen for a period of time and are used to compute the target value. Periodically, the weights of the policy network are copied over to the target network. This makes the target value much more stable, which greatly improves the stability of training.

**The DQN Loss Function:** The network is trained by minimizing the mean squared error between the predicted Q-value and the target Q-value:

`Loss = E [ (r + γ * max_{a'} Q(s', a'; θ_target) - Q(s, a; θ_policy))² ]`

**Interview Insight:** DQN was a landmark achievement. Be sure to understand *why* experience replay and fixed Q-targets are so important. They are not just minor tricks; they are fundamental solutions to the core problems that arise when you try to combine deep learning with the bootstrapping nature of Q-learning.