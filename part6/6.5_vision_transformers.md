# 5.5. Vision Transformers (ViT)

## 1. Introduction: Transformers for Image Recognition

For a long time, CNNs were the undisputed kings of computer vision. The conventional wisdom was that the inductive biases of a CNN (like local connectivity and translation equivariance) were essential for processing images. The Vision Transformer (ViT), introduced by Google researchers in 2020, challenged this assumption. It demonstrated that a pure Transformer architecture, with minimal modifications, could achieve state-of-the-art results on image classification tasks, provided it was pre-trained on a sufficiently large dataset.

ViT represents a major paradigm shift, showing that the general-purpose learning capabilities of Transformers could be successfully applied to the vision domain, without the need for convolution-specific architectures.

---

## 2. The ViT Architecture: Treating an Image as a Sequence

The core challenge in applying a Transformer to an image is that Transformers expect a 1D sequence of tokens, while an image is a 2D grid of pixels. The ViT solves this with a simple and effective strategy:

1.  **Image Patching:** The input image is split into a grid of fixed-size, non-overlapping patches. For example, a 224x224 image might be split into 196 patches of 16x16 pixels each.

2.  **Patch Embedding:** Each of these image patches is "flattened" into a 1D vector and then linearly projected into a fixed-size embedding. This is analogous to the word embeddings in an NLP Transformer. The result is a sequence of patch embeddings.

3.  **Positional Embeddings:** Just like in NLP, the standard Transformer has no inherent sense of position. To retain the spatial information of the patches, a learnable 1D positional embedding is added to each patch embedding.

4.  **Classification Token (`[CLS]`):** Inspired by BERT, a special learnable `[CLS]` token is prepended to the sequence of patch embeddings. The final hidden state corresponding to this token will be used as the aggregate representation of the entire image for classification.

5.  **Transformer Encoder:** This sequence of embeddings (the `[CLS]` token plus the patch embeddings) is then fed directly into a standard Transformer encoder, which consists of a stack of multi-head self-attention and feed-forward layers.

6.  **Classification Head:** The final hidden state of the `[CLS]` token is passed to a simple MLP head (typically one hidden layer) which produces the final class prediction.

---

## 3. Key Differences and Insights

*   **Inductive Bias:**
    *   **CNNs:** Have strong inductive biases built into their architecture. The convolutional operation assumes that local pixel relationships are important and that the same features can appear anywhere in the image (translation equivariance). This makes them very data-efficient.
    *   **ViT:** Has a much weaker inductive bias. It does not assume locality. The self-attention mechanism allows any patch to interact with any other patch, regardless of their position. This makes the ViT more flexible but also more data-hungry.

*   **Data and Pre-training:**
    *   When trained on smaller datasets (like ImageNet-1k), ViTs generally perform worse than modern CNNs like ResNet. They don't have enough data to learn the basic image features that are hard-coded into CNNs.
    *   However, when pre-trained on very large, private datasets (like Google's JFT-300M, which has 300 million images), ViTs can outperform state-of-the-art CNNs when fine-tuned on smaller, public benchmarks. The ViT's flexibility allows it to learn powerful representations from scratch, given enough data.

**Interview Insight:** The most important concept to convey about ViT is the **trade-off between inductive bias and data scale**. CNNs are highly effective because their architecture encodes strong, correct assumptions about the nature of images. This makes them learn well on smaller datasets. ViTs make fewer assumptions, which gives them a higher ceiling for performance, but they require a massive amount of data to reach that ceiling. The success of ViT showed that, with sufficient scale, a general-purpose learning architecture could discover the same kinds of representations that were previously hand-engineered into CNNs.