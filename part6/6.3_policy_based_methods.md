# 6.3. Policy-Based Methods (REINFORCE, Actor-Critic)

## 1. Introduction: Learning a Policy Directly

While value-based methods learn a value function and then derive a policy from it, **policy-based methods** learn a parameterized policy directly, without needing to learn a value function first. The policy is represented by a function (typically a neural network) `π(a|s; θ)`, which outputs a probability distribution over actions given a state, where `θ` are the parameters (weights) of the network.

**Advantages of Policy-Based Methods:**
*   **Continuous Action Spaces:** They can handle continuous action spaces naturally, whereas value-based methods like Q-learning struggle with this.
*   **Stochastic Policies:** They can learn stochastic (random) policies, which can be advantageous in some environments.
*   **Simpler:** In some cases, the policy function can be simpler to approximate than the value function.

The goal is to adjust the policy parameters `θ` to maximize the expected return. This is done using **policy gradient** methods.

---

## 2. REINFORCE: The Foundational Policy Gradient Algorithm

REINFORCE is the simplest and most fundamental policy gradient algorithm. It works by running the policy for an entire episode, collecting the sequence of states, actions, and rewards. Then, it updates the policy parameters in the direction that makes "good" actions more likely.

**The Core Idea:**
The policy gradient theorem provides the gradient of the expected return with respect to the policy parameters:

`∇_θ J(θ) = E [ G_t * ∇_θ log π(a_t | s_t; θ) ]`

Let's break this down:
*   `J(θ)`: The expected return of the policy parameterized by `θ`.
*   `∇_θ`: The gradient with respect to `θ`.
*   `G_t`: The **return**, which is the total discounted reward from timestep `t` to the end of the episode.
*   `∇_θ log π(a_t | s_t; θ)`: The gradient of the log-probability of taking action `a_t` in state `s_t`.

**The Update Rule:** After an episode, for each timestep `t`, we update `θ`:

`θ ← θ + α * G_t * ∇_θ log π(a_t | s_t; θ)`

This update has a very intuitive interpretation:
*   If `G_t` is high (a good return), we push `θ` in the direction that increases the probability of taking action `a_t` in state `s_t`. We are "reinforcing" good actions.
*   If `G_t` is low (a bad return), we push `θ` in the opposite direction, making action `a_t` less likely in the future.

**Limitations:**
*   **High Variance:** The return `G_t` can have very high variance, which makes the training process unstable and slow to converge. The return from a single episode might not be representative of the true quality of the actions taken.
*   **On-Policy:** REINFORCE is an on-policy algorithm. The data used for updates must be collected using the most recent version of the policy. This is sample-inefficient.

---

## 3. Actor-Critic Methods: The Best of Both Worlds

Actor-Critic methods combine the strengths of both policy-based and value-based methods to create more stable and efficient algorithms. They consist of two components, which are two separate neural networks:

1.  **The Actor:** This is the policy network. It controls how the agent behaves by learning a policy `π(a|s; θ)`.
2.  **The Critic:** This is the value network. It learns a value function (either the state-value function `V(s; w)` or the action-value function `Q(s, a; w)`) and "criticizes" the actions taken by the actor.

**How it works:**
The actor takes an action. The critic observes the outcome and evaluates how good that action was. This evaluation is then used to update the actor's policy.

**The Key Improvement: Reducing Variance**
Instead of using the noisy, full return `G_t` to scale the policy gradient (as in REINFORCE), Actor-Critic methods use the **TD error** from the critic.

The policy gradient update becomes:
`θ ← θ + α * TD_error * ∇_θ log π(a_t | s_t; θ)`

The TD error is typically calculated as: `TD_error = r_t + γ * V(s_{t+1}; w) - V(s_t; w)`

This TD error is a much lower-variance estimate of how much better or worse the action `a_t` was than expected. This leads to much more stable and faster training compared to REINFORCE.

**Interview Insight:** Actor-Critic methods are the foundation for most state-of-the-art deep RL algorithms (e.g., A2C/A3C, DDPG, SAC, PPO). The core idea is to use a learned value function (the critic) to provide a better, lower-variance learning signal for the policy (the actor). This hybrid approach elegantly combines the ability of policy gradients to handle complex action spaces with the stability and efficiency of value-based bootstrapping.