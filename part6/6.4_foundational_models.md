# 5.4. Foundational Models: BERT and GPT

## 1. Introduction: The Age of Pre-training and Fine-tuning

The original Transformer was a powerful architecture, but its true impact was unlocked by a new paradigm: **pre-training and fine-tuning**. The idea is to first train a large Transformer model on a massive amount of unlabeled text data (pre-training) to learn a general understanding of language. Then, this pre-trained model can be quickly adapted (fine-tuned) for specific downstream tasks (like sentiment analysis or question answering) using a much smaller amount of labeled data.

BERT and GPT are two of the most influential foundational models that pioneered this approach. They are both based on the Transformer architecture, but they make different architectural choices that make them suitable for different kinds of tasks.

---

## 2. BERT: Bidirectional Encoder Representations from Transformers

BERT, developed by Google AI, is a model designed for **Natural Language Understanding (NLU)** tasks. Its key innovation is that it learns a **deeply bidirectional** representation of language.

*   **Architecture:** BERT uses only the **encoder** stack from the Transformer architecture. It is designed to take a sequence of text and produce a rich, context-aware representation for each word.

*   **Bidirectionality:** Unlike previous models that were either unidirectional (left-to-right) or shallowly bidirectional, BERT's self-attention mechanism allows every word to attend to every other word in the sequence simultaneously. This allows it to build a representation of a word that is conditioned on both its left and right context.

*   **Pre-training Objectives:** To enable this bidirectionality, BERT is pre-trained on two novel tasks:
    1.  **Masked Language Model (MLM):** Before feeding a sentence into the model, 15% of the words are randomly masked (replaced with a `[MASK]` token). The model's task is to predict the original identity of these masked words based on the surrounding unmasked words.
    2.  **Next Sentence Prediction (NSP):** The model is given two sentences, A and B, and must predict whether sentence B is the actual sentence that follows A in the original text, or if it's just a random sentence. This helps the model learn about sentence-level relationships.

*   **Fine-tuning:** For downstream tasks like sentiment analysis or question answering, you typically add a small classification layer on top of BERT's final hidden state for the `[CLS]` token (a special token prepended to every input) and fine-tune the entire model.

**Interview Insight:** BERT is an **encoder-only** model that excels at tasks requiring a deep understanding of the context of a sentence. It's not a generative model; it's designed to produce rich embeddings.

---

## 3. GPT: Generative Pre-trained Transformer

GPT, from OpenAI, is a model designed for **Natural Language Generation (NLG)** tasks.

*   **Architecture:** GPT uses only the **decoder** stack from the Transformer architecture.

*   **Unidirectionality (Auto-regressive):** GPT is an auto-regressive model, meaning it generates text one word at a time, from left to right. The self-attention mechanism in GPT is **masked** to prevent a position from attending to subsequent positions. This is crucial for its generative capabilities; it can only use the previously generated words as context to predict the next word.

*   **Pre-training Objective:** GPT is pre-trained on a standard **language modeling** objective: predict the next word in a sequence given all the previous words. It is trained on a massive corpus of text to learn the patterns, grammar, and facts of the language.

*   **Fine-tuning & Prompting:** Early versions of GPT were fine-tuned for specific tasks. However, with the advent of very large models (like GPT-3 and beyond), a new paradigm of **in-context learning** or **prompting** has emerged. Instead of fine-tuning the model's weights, you simply provide a natural language prompt that describes the task, sometimes with a few examples, and the model generates the desired output directly.

**Interview Insight:** GPT is a **decoder-only** model that excels at generative tasks. Its auto-regressive nature and masked self-attention are its defining features.

---

## 4. BERT vs. GPT: The Key Difference

| Feature        | BERT                                       | GPT                                        |
|----------------|--------------------------------------------|--------------------------------------------|
| **Architecture** | Encoder-only                               | Decoder-only                               |
| **Attention**  | Bidirectional (unmasked self-attention)    | Unidirectional (masked self-attention)     |
| **Primary Use**  | NLU (Classification, NER, QA)              | NLG (Text Generation, Summarization)       |
| **Analogy**      | A student filling in the blanks in a text. | A student writing the next sentence of a story. |

In short:
*   Use a **BERT-like** model when you need to create rich representations of text for understanding-based tasks.
*   Use a **GPT-like** model when you need to generate new text.