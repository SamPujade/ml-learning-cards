# 2.1. Neural Networks & Multi-Layer Perceptrons (MLPs)

## 1. Introduction: The Building Blocks of Deep Learning

Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the structure and function of the biological brain. They are the foundational technology behind the field of deep learning. By stacking layers of simple computational units (neurons), neural networks can learn to approximate incredibly complex, non-linear functions, making them powerful tools for tasks ranging from image recognition to natural language processing.

The Multi-Layer Perceptron (MLP) is the classic, quintessential form of a neural network. Understanding the MLP is the first step to understanding more complex deep learning architectures like CNNs and RNNs.

---

## 2. The Artificial Neuron (Perceptron)

The basic unit of a neural network is the neuron, also known as a perceptron. A single neuron performs a simple two-step computation:

1.  **Weighted Sum:** It takes a set of numerical inputs, multiplies each input by a weight, and sums them up. A bias term is also added to this sum. The weights and the bias are the parameters that the model learns during training.
    `z = (w₁x₁ + w₂x₂ + ... + wₙxₙ) + b = wᵀx + b`

2.  **Activation:** The result of the weighted sum (`z`) is then passed through a non-linear **activation function** (`g`). This non-linearity is crucial; without it, a multi-layer network would be mathematically equivalent to a single-layer network and would be unable to learn complex patterns.
    `a = g(z)`

A single neuron, by itself, can act as a linear classifier (like logistic regression). The real power of neural networks comes from combining many of these neurons into layers.

---

## 3. The Multi-Layer Perceptron (MLP)

An MLP consists of multiple layers of neurons, fully connected to each other. It has three types of layers:

*   **Input Layer:** This layer receives the raw input features. The number of neurons in the input layer is equal to the number of features in the dataset. It doesn't perform any computation; it simply passes the data to the first hidden layer.

*   **Hidden Layers:** These are the layers between the input and output layers. An MLP can have one or more hidden layers. These layers are what allow the network to learn complex, hierarchical features from the data. The first hidden layer might learn simple patterns (like edges in an image), and subsequent layers can combine these to learn more complex patterns (like shapes, and then objects). The "deep" in deep learning refers to having many hidden layers.

*   **Output Layer:** This layer produces the final prediction. The number of neurons and the activation function used in the output layer depend on the type of problem:
    *   **Binary Classification:** 1 neuron with a sigmoid activation function (to output a probability between 0 and 1).
    *   **Multi-class Classification:** N neurons (where N is the number of classes) with a softmax activation function (to output a probability distribution over the classes).
    *   **Regression:** 1 neuron with a linear (or no) activation function (to output a continuous value).

**Fully Connected:** In an MLP, each neuron in a given layer is connected to every neuron in the subsequent layer. This is why MLPs are also known as fully connected networks.

---

## 4. How MLPs Learn: Forward and Backward Propagation

Training an MLP involves a two-phase process that is repeated for many epochs:

1.  **Forward Propagation:** A batch of training data is fed into the input layer. The data flows through the network, layer by layer, with each neuron performing its weighted sum and activation. This process continues until the output layer produces a prediction.

2.  **Backward Propagation (Backpropagation):**
    *   **Loss Calculation:** The model's predictions are compared to the true labels using a loss function (e.g., cross-entropy for classification, MSE for regression). The loss quantifies how wrong the model's predictions are.
    *   **Gradient Calculation:** Backpropagation is the algorithm used to calculate the gradient of the loss function with respect to each weight and bias in the network. It works by applying the chain rule of calculus, starting from the output layer and moving backward through the network.
    *   **Parameter Update:** The gradients are then used by an optimization algorithm (like Adam or SGD) to update the weights and biases in the direction that minimizes the loss.

**Interview Insight:** The Universal Approximation Theorem is a key concept to mention when discussing MLPs. It states that an MLP with a single hidden layer containing a finite number of neurons can approximate any continuous function to an arbitrary degree of accuracy. This provides the theoretical justification for the power of neural networks.