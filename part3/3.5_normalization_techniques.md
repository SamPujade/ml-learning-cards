# Normalization Techniques

In deep neural networks, normalization layers are crucial for training deeper models, accelerating convergence, and improving generalization. They work by normalizing the activations from a previous layer before they are passed to the next.

## Batch Normalization (BatchNorm)

Batch Normalization is the most common normalization technique. It normalizes activations across the batch dimension.

### How it Works

For a mini-batch of activations, BatchNorm computes the mean and variance of the activations over the mini-batch. It then normalizes the activations and applies a learnable affine transformation (scale and shift).

1.  **Calculate mean and variance:** Compute the mean and variance for each feature over the mini-batch.
2.  **Normalize:** Subtract the mean and divide by the standard deviation.
3.  **Scale and shift:** Multiply by a learnable parameter `gamma` and add a learnable parameter `beta`.

### Key Characteristics

-   **Depends on batch size:** The statistics are batch-dependent. This can be an issue with small batch sizes.
-   **Has a regularizing effect:** The noise from the batch statistics acts as a regularizer.
-   **Different behavior during training and inference:** During training, it uses batch statistics. During inference, it uses running averages of mean and variance collected during training.

## Layer Normalization (LayerNorm)

Layer Normalization normalizes activations across the feature dimension for a single training example.

### How it Works

For each training example in a batch, LayerNorm computes the mean and variance of all activations in that layer.

### Key Characteristics

-   **Independent of batch size:** Its calculations do not involve other examples in the batch, so it works well with any batch size.
-   **Common in RNNs and Transformers:** It's very effective in recurrent networks and transformers where the sequence length can vary.

## Instance Normalization (InstanceNorm)

Instance Normalization is like a simplified version of Layer Normalization. It normalizes each channel of each training example independently.

### How it Works

For each training example and each channel, it computes the mean and variance and normalizes the activations.

### Key Characteristics

-   **Independent of batch size.**
-   **Common in style transfer:** It can "wash out" contrast information from an image, which is useful for generative tasks like style transfer.

## Group Normalization (GroupNorm)

Group Normalization is a compromise between InstanceNorm and LayerNorm. It divides the channels into groups and normalizes within each group.

### How it Works

1.  **Group channels:** The channels are divided into a number of groups.
2.  **Normalize:** For each training example, it computes the mean and variance within each group of channels and normalizes.

### Key Characteristics

-   **Independent of batch size.**
-   **Performance is stable over a wide range of batch sizes.**
-   **Good general-purpose alternative to BatchNorm.**

## Comparison

| Technique           | Normalizes Over...                               | Batch Dependent? |
| ------------------- | ------------------------------------------------ | ---------------- |
| **Batch Norm**      | Batch, Height, Width (for a given channel)       | Yes              |
| **Layer Norm**      | Channels, Height, Width (for a given example)    | No               |
| **Instance Norm**   | Height, Width (for a given channel and example)  | No               |
| **Group Norm**      | Groups of channels, Height, Width (for a given example) | No               |
