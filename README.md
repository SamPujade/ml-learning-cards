# AI, ML, & DL Learning Cards

Welcome to this curated collection of learning cards, designed for senior AI scientists preparing for technical interviews at major tech companies. Each card provides a high-level overview of a key topic, focusing on the essential concepts and their practical applications.

## Table of Contents

### Part 1: Machine Learning Foundations
*   [1.1. Core Concepts: Supervised, Unsupervised, and Reinforcement Learning](./part1/1.1_core_concepts.md)
*   [1.2. The Bias-Variance Tradeoff](./part1/1.2_bias_variance_tradeoff.md)
*   [1.3. Overfitting and Regularization (L1, L2, Dropout)](./part1/1.3_overfitting_regularization.md)
*   [1.4. Optimization Algorithms (Gradient Descent, SGD, Adam)](./part1/1.4_optimization_algorithms.md)
*   [1.5. Evaluation Metrics for Classification & Regression](./part1/1.5_evaluation_metrics.md)

### Part 2: Deep Learning Fundamentals
*   [2.1. Neural Networks & Multi-Layer Perceptrons (MLPs)](./part2/2.1_neural_networks.md)
*   [2.2. Activation Functions (ReLU, Sigmoid, Tanh)](./part2/2.2_activation_functions.md)
*   [2.3. Backpropagation and Automatic Differentiation](./part2/2.3_backpropagation.md)
*   [2.4. Weight Initialization Techniques](./part2/2.4_weight_initialization.md)

### Part 3: Convolutional Neural Networks (CNNs)
*   [3.1. Convolutional & Pooling Layers](./part3/3.1_convolutional_pooling.md)
*   [3.2. Classic CNN Architectures (LeNet, AlexNet, VGG, ResNet)](./part3/3.2_classic_architectures.md)
*   [3.3. Advanced CNN Applications (Object Detection, Segmentation)](./part3/3.3_advanced_applications.md)

### Part 4: Sequential Models
*   [4.1. Recurrent Neural Networks (RNNs) & The Vanishing/Exploding Gradient Problem](./part4/4.1_rnns.md)
*   [4.2. Long Short-Term Memory (LSTM) Networks](./part4/4.2_lstms.md)
*   [4.3. Gated Recurrent Units (GRUs)](./part4/4.3_grus.md)

### Part 5: The Transformer Architecture
*   [5.1. The Attention Mechanism](./part5/5.1_attention_mechanism.md)
*   [5.2. Self-Attention and Multi-Head Attention](./part5/5.2_self_attention.md)
*   [5.3. The Transformer Encoder-Decoder Model](./part5/5.3_transformer_model.md)
*   [5.4. Foundational Models (BERT, GPT)](./part5/5.4_foundational_models.md)
*   [5.5. Vision Transformers (ViT)](./part5/5.5_vision_transformers.md)

### Part 6: Reinforcement Learning (RL)
*   [6.1. Markov Decision Processes (MDPs)](./part6/6.1_mdps.md)
*   [6.2. Value-Based Methods (Q-Learning, DQN)](./part6/6.2_value_based_methods.md)
*   [6.3. Policy-Based Methods (REINFORCE, Actor-Critic)](./part6/6.3_policy_based_methods.md)

### Part 7: Advanced & Specialized Topics
*   [7.1. Generative Models (GANs, VAEs, Diffusion Models)](./part7/7.1_generative_models.md)
*   [7.2. Graph Neural Networks (GNNs)](./part7/7.2_gnns.md)
*   [7.3. Self-Supervised and Unsupervised Learning](./part7/7.3_self_supervised_learning.md)
*   [7.4. MLOps: Productionizing ML Models](./part7/7.4_mlops.md)
